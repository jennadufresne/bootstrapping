---
title: "Bootstrapping"
author: "Caelan Bryan, Jenna Dufresne, Jamielee Jimenez Perez"
date: "Fall 2022"
output:
  html_document:
    df_print: paged
  pdf_document: default
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
nocite: | 
  @efron1
  @efron2
  @introduction
  @assumptions
  @withr
  @chavez_2022
  @worldbank
  @mastersindata
---

## Literature Review

Bootstrap Methods: Another Look at the Jackknife [@efron1] is credited as being 
the creation of bootstrapping by author B. Efron. It starts by explaining that
bootstrapping is a step above the jackknife method, and that the jackknife is
a linear expansion for estimating the bootstrap. Starting in the introduction,
it is stated that the bootstrap is shown to estimate the variance of the sample
median, which is an area that the jackknife fails at. It is also mentioned that
the bootstrap does well at estimating the error rates in certain problems, which
outperforms other non-parametric estimation methods. The problem attempting to
be solved is estimating the sampling distribution based on the observed data.
The idea behind the bootstrap method is listed in three parts. First, construct
the sample probability distribution. Second, draw a random sample of size n. 
Lastly, approximate the sampling distribution by the bootstrap distribution. It
is mentioned that that the difficult part of bootstrapping is calculating the 
bootstrap distribution and three methods are given to accomplish this; direct
Monte Carlo approximations, and Taylor series expansion methods. A few
applications are listed. These include estimating the median, error rate 
estimation in discrimination analysis, relationship with the jackknife,
Wilcoxon's statistic, and regression models. Finally, a list of some remarks
regarding the bootstrap method are listed. Some important ideas listed are that
the calculation of the bootstrap distribution using the Monte Carl method is
easy to implement on the computer and that the bootstrap and jackknife provide
approximate frequency statements and not approximate likelihood statements.

Nonparametric Estimates of Standard Error: The Jackknife, the Bootstrap, and
Other Methods [@efron2] starts by giving some background on what the purpose of 
the paper is. In this case, we want to estimate the standard error based on the 
data. This is normally done through parametric modeling methods, but in the 
paper it is being done through nonparametric methods. All methods are being 
tested using the same scenario which is bivariate normal distribution. This 
paper has four points. They are to describe the various methods, show how the 
methods derive from the same idea, relate the methods to each other, and finally 
show how the methods perform differently even though they are similar. The Monte 
Carlo experiment that the data was obtained from is then described. The 
bootstrap method is the first that is looked at. The steps are described to 
obtain the bootstrap samples. Random samples are created from the original 
sample and the bootstrap estimate is obtained. This process is than repeated 
multiple time to achieve enough samples to accurately estimate the standard 
error. 128 and 512 total samples were used separately, and while the 512 total 
sample provided a slightly more accurate result, the increase was minimal. 
on this result, the size of N samples is not overly important past 50-100. A 
smoothed bootstrap method is than looked at. This produced by compromising the 
normal theory maximum, likelihood estimate and the nonparametric maximum 
likelihood estimate. The results of the smoothed bootstrap method were overall 
better than those in the on-smoothed bootstrap method. Next, the jackknife, 
infinitesimal jackknife, half-sampling, and random sub sampling methods are 
discussed. All results are displayed in a table containing th estimated values, 
standard deviation, and confidence interval for each estimate. Overall, the 
bootstrap method produced the results that closest matches the theoretical 
values.

In Bootstrapping - An Introduction And Its Applications In Statistics 
[@introduction] the authors take a high-level approach to introducing the reader 
to bootstrapping along with applications to different parts of statistics. The 
introduction begins to explain what bootstrapping is and how it is accomplished. 
They make use of an example to help explain what bootstrapping is. In this case, 
they look at performing analysis on the population of the United States. It 
would be difficult, costly, and timely to sample the entire population so 
instead you might sample a smaller subset of the population and create new 
bootstrap samples using replacement. In this sense the bootstrap samples might 
contain duplicates or even omit certain responses from the original sample. 
Doing so allows interpretation of the entire population based on a subset of the 
population. One of the first applications looked at is estimation of means and 
confidence intervals for the mean. Another application is constructing 
confidence intervals for regression coefficients. The authors also look at 
regression models, case re-sampling, estimating the distribution of sample mean, 
Bayesian bootstrap, smooth bootstrap, parametric bootstrap, and re-sampling 
residuals as applications of bootstrapping. Lastly, some of the advantages and 
disadvantages are listed.  One advantage is the simplicity of deriving estimates 
of standard errors and confidence intervals where a disadvantage would be that 
the result still depends on the original sample.

In The Importance of Discussing Assumptions when Teaching Bootstrapping
[@assumptions] the authors spend time ensuring that the readers understand when 
it is appropriate to use bootstrapping methods by presenting the math along with 
experimental results. The paper starts with an introduction to what 
bootstrapping is. They also mention that bootstrapping has increased in 
popularity since it’s inception with applications in linear regression and 
neural networks among other fields. The methods focused on during the paper are 
studentized, basic, and percentile bootstrap intervals and their hypothesis 
tests. The next section focuses on why it’s important to teach statistical 
computing and bootstrapping. One feature relevant to bootstrapping is that 
students understandings of confidence intervals and statistical inference relies 
on their understanding of sampling distributions. Next, some of the assumptions 
for bootstrapping are discussed. The assumptions were split based on interval 
estimation and hypothesis testing. For both cases, the largest assumption is 
that the distribution can be made approximately pivotal through shifting or 
studentization. Lastly, simulation-based performance was evaluated. This was 
done by using the metrics coverage proportion, significance level, and power. 
The most important results were that when the assumptions are broken, there can 
be differences in the performance of the different bootstrapping methods. There 
also was not a improvement between bootstrapping and other methods whose 
assumptions were also broken. The smaller the sample size and non-normalcy also 
impacted the performance of the methods. Lastly, an R package was created, which
encompasses the functions used throughout the paper, that can be used to create 
intervals using bootstrapping methods.

Bootstrapping with R to make generalized inference for regression model [@withr] 
looks at a specific application of bootstrapping, validating a generalized 
regression model to make generalization of statistical inference to different 
cases outside of the original sample. In the introduction, the authors explain 
the different types of regression models and explain the different ways to 
complete model validation. Some of those listed include cross-validation, 
Jackknife, and bootstrap methods. The idea of the bootstrap method is then also 
explained. The methodology and design of the experiment is then outlined. In 
this case, the original observations will be resampled leading to a set of 
bootstrap samples. The mean estimate and regression coefficient estimates can 
then be found for each bootstrap sample. Finally, confidence intervals can be 
created for the mean, regression coefficients, and standard errors for both the 
mean and regression coefficients. A table is provided to outline the values 
received from the original sample and bootstrap samples. Finally, the results 
are discussed in the conclusion. Based on the results of this experiment, the 
values found are very similar among the original and bootstrap samples, although 
the confidence intervals for the bootstrap samples are often wider. Some 
advantages to bootstrapping are also listed.

In the article, Bootstrapping [@mastersindata], it first begins with a statement 
from Benjamin Zimmer. He believes the origin of bootstrapping was impossible to 
do. This idea evolved from a man trying to jump a fence by the force of pulling 
up on his bootstraps. Fortunately, statistically bootstrapping is possible. 
Bootstrapping is taking a population, creating a small collection of data by 
using replacement and randomly resampling to analyze. The idea of replacement is 
very important when discussing bootstrapping. When replacement happens in 
bootstrapping it means every item that is drawn from the population, the same 
item exists in the sample. The importance behind sampling and replacement is 
when each sample or subset is made there will be statistical measurements made 
on each set or on all the sets together. Once the measurements are done the data 
is ready to plot. After the data is plotted analysis can be done. Furthermore, 
inferences can be made on the population as a whole. As the article continues, 
dives in to the importance of machine learning and how to implement 
bootstrapping in python. To further understand bootstrapping the article gives 
an example and states advantages and disadvantages.

In the article, Advanced statistics: Bootstrapping confidence intervals for 
statistics with “difficult” distributions explains the steps in bootstrapping to
estimate confidence intervals for the two software packages, SAS and Stata. In 
1979 bootstrapping was first introduced as a statistical technique allowing 
researchers to make inferences from data without making assumptions about 
distributions. In this case there are two distributions to take into account: 
probability function (normal, binomial, or Poisson) or distribution of 
statistics (median) calculated from the data. While estimating the confidence
intervals using bootstrapping the article givesthe following steps: first, 
sample space is found with replacement with the same number of variables in the
original data set. Then, step one is preformed until researchers are satisfied 
with the number of data sets. The following step is deciding the descriptive 
statistic and computing it on each data set. Next, a confidence interval is 
calculated from the collection of values. After all these steps are done, there 
are multiple different options for computing the confidence intervals. For 
example, the normal approximation method, percentile method, bias-corrected 
method and approximate bootstrapping confidence method. Throughout the article 
it states different examples on determining confidence intervals around various 
statical distribution and using software to help make inferences about it.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(knitr)
```

## Data Description

The dataset we will be using to explore Bootstrapping is the "Causes of Death -
Our World in Data" dataset from kaggle [@chavez_2022]. The causes of death 
dataset was also expanded with population data from the world bank [@worldbank].

```{r}
# Import the data
bootstrapping_data_cleaned = read.csv('bootstrapping_data_cleaned.csv', 
                                      stringsAsFactors = TRUE)
```

The raw data contains a multitude of death statistics broken down by the 
continent, region, country, and territory. The statistics for cause of death
are given in number of deaths and split up by the cause of death. Data is
presented over the span of multiple years. The population dataset from the
world bank is a list of countries, regions, and territories but contains the
population for each year from 1960 to 2021 where available.

```{r}
# Display a sample of the data
head(bootstrapping_data_cleaned)
```

To get more meaningful numbers, the population for each year 1990 through 2019
was populated into the causes of death dataset. In order to perform a more
predictable experiment, the causes of death dataset was cleaned by first removing
all non-country entries. The number of executions and terrorism deaths columns
were also removed due to a lack of data for the majority of countries. As a
final note, the Vatican and Liechtenstein are the two countries that did not
have cause of death statistics available in the dataset.

The columns in the cleaned dataset are as follows:

```{r echo=FALSE}
Name <- c('Entity', 
          'Population', 
          'Code', 
          'Year', 
          'Deaths_Meningitis', 
          'Deaths_Neoplasms',
          'Deaths_FireHeatHotSubstances', 
          'Deaths_Malaria', 
          'Deaths_Drowning', 
          'Deaths_InterpersonalViolence', 
          'Deaths_HIVAIDS', 
          'Deaths_DrugUseDisorders',
          'Deaths_Tuberculosis', 
          'Deaths_RoadInjuries', 
          'Deaths_MaternalDisorders', 
          'Deaths_LowerRespiratoryInfections',
          'Deaths_NeonatalDisorders',
          'Deaths_AlcoholUseDisorders',
          'Deaths_ExposureToForcesOfNature',
          'Deaths_DiarrhealDiseases',
          'Deaths_EnvironmentalHeatAndColdExposure',
          'Deaths_NutritionalDeficiencies',
          'Deaths_Selfharm',
          'Deaths_ConflictAndTerrorism',
          'Deaths_DiabetesMellitus',
          'Deaths_Poisonings',
          'Deaths_ProteinEnergyMalnutrition',
          'Deaths_CardiovascularDiseases',
          'Deaths_ChronicKidneyDisease',
          'Deaths_ChronicRespiratoryDiseases',
          'Deaths_CirrhosisOtherChronicLiverDiseases',
          'Deaths_DigestiveDiseases',
          'Deaths_AcuteHepatitis',
          'Deaths_AlzheimersDiseaseOtherDementias',
          'Deaths_ParkinsonsDisease')
Description <- c('Name of Country',                    
                 'Population of Country at Specific Year',                    
                 'Three Letter Country Code',                    
                 'Year for Causes of Deaths',              
                 'Number of Deaths Caused by Meningitis',                
                 'Number of Deaths Caused by Neoplasms',         
                 'Number of Deaths Caused by Fire, Heat, or Hot Substances',                
                 'Number of Deaths Caused by Malaria',      
                 'Number of Deaths Caused by Drowning',                
                 'Number of Deaths Caused by Interpersonal Violence',      
                 'Number of Deaths Caused by HIV/AIDS',                
                 'Number of Deaths Caused by Drug Use Disorders',      
                 'Number of Deaths Caused by Tuberculosis',                
                 'Number of Deaths Caused by Road Injuries',      
                 'Number of Deaths Caused by Maternal Disorders',                
                 'Number of Deaths Caused by Lower Respiratory Infections',      
                 'Number of Deaths Caused by Neonatal Disorders',                
                 'Number of Deaths Caused by Alcohol Use Disorders',      
                 'Number of Deaths Caused by Exposure to Forces of Nature',                
                 'Number of Deaths Caused by Diarrheal Diseases',      
                 'Number of Deaths Caused by Environmental Heat and Cold Exposure',                
                 'Number of Deaths Caused by Nutritional Deficiencies',      
                 'Number of Deaths Caused by Self-Harm',                
                 'Number of Deaths Caused by Conflict and Terrorism',      
                 'Number of Deaths Caused by Diabetes Mellitus',                
                 'Number of Deaths Caused by Poisoning',      
                 'Number of Deaths Caused by Protein Energy Malnutrition',                
                 'Number of Deaths Caused by Cardiovascular Diseases',      
                 'Number of Deaths Caused by Chronic Kidney Disease',                
                 'Number of Deaths Caused by Chronic Respiratory Diseases',      
                 'Number of Deaths Caused by Cirrhosis or Other Chronic Liver Diseases',                
                 'Number of Deaths Caused by Digestive Diseases',      
                 'Number of Deaths Caused by Acute Hepatitis',                
                 'Number of Deaths Caused by Alzheimers Disease or Other Dementias',      
                 'Number of Deaths Caused by Parkinsons Disease')                        
Type <- c('Nominal',  
          'Discrete',   
          'Nominal',
          'Nominal',   
          'Discrete',  
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete',
          'Discrete')
variable_data <- data.frame(Name, Description, Type)
kable(variable_data)
```

In order to fit our data into our experiments below we will need to further
modify the dataset. The first thing we want to do is add a new column that is
sum of all causes of death columns, or the total number of reported deaths. 
Second, we will extract only the data from the most recent year, in the case of 
this dataset it is 2019. Next, we are only going to look at a subset of the 
available causes of death. To make things easy we will select the most common 
causes of death, cardiovascular diseases and neoplasms (new and abnormal growth 
of tissue in some part of the body). Lastly, to normalize the data between 
countries, we will use the population to obtain the rate of death caused by 
cardiovascular diseases and neoplasms.

```{r}
# Create the total number of reported deaths column
bootstrapping_data_cleaned$Number_Of_Deaths <- rowSums(bootstrapping_data_cleaned[,(5:35)])

# Filter down the data to just include 2019 and just the columns we want
boot_data = bootstrapping_data_cleaned[bootstrapping_data_cleaned$Year == '2019', 
                                       c('Entity', 'Population', 
                                         'Number_Of_Deaths', 'Deaths_Neoplasms',
                                         'Deaths_CardiovascularDiseases')]

# Create the rates columns
boot_data$Deaths_CardiovascularDiseasesRate <- boot_data$Deaths_CardiovascularDiseases / boot_data$Number_Of_Deaths
boot_data$Deaths_NeoplasmsRate <- boot_data$Deaths_Neoplasms / boot_data$Number_Of_Deaths
boot_data$Deaths_CardiovascularDiseasesPopulationRate <- boot_data$Deaths_CardiovascularDiseases / boot_data$Population
boot_data$Deaths_NeoplasmsPopulationRate <- boot_data$Deaths_Neoplasms / boot_data$Population
```

Now that we have all the data needed to move forward we can find population 
descriptive statistics that will be useful in comparing against our estimated
population statistics from the bootstrapping method. We can also visualize
the data to check that it follows a normal distribution using a histogram.

```{r}
# Find cardiovascular disease rate statistics
boot_means <- boot_data %>% summarize(mean_cardiovasculardiseasesrate = mean(Deaths_CardiovascularDiseasesRate),
                                      mean_neoplasmsrate = mean(Deaths_NeoplasmsRate),
                                      sd_cardiovasculardiseasesrate = sd(Deaths_CardiovascularDiseasesRate),
                                      sd_neoplasmsrate = sd(Deaths_NeoplasmsRate))

```

We find that the average rate of deaths caused by cardiovascular diseases across
the entire population is `r round(boot_means$mean_cardiovasculardiseasesrate, 4)`,
or `r round(boot_means$mean_cardiovasculardiseasesrate * 100, 2)`%, with a 
standard deviation of `r round(boot_means$sd_cardiovasculardiseasesrate, 4)`, or
`r round(boot_means$sd_cardiovasculardiseasesrate * 100, 2)`%.

We find that the average rate of deaths caused by neoplasms across the entire 
population is `r round(boot_means$mean_neoplasmsrate, 4)`, or 
`r round(boot_means$mean_neoplasmsrate * 100, 2)`%, with a standard deviation of 
`r round(boot_means$sd_neoplasmsrate, 4)`, or
`r round(boot_means$sd_neoplasmsrate * 100, 2)`%.

```{r}
# Display a histogram of the cardiovascular diseases rate
ggplot(boot_data, aes(x = Deaths_CardiovascularDiseasesRate)) +
geom_histogram()
```

```{r}
# Display a histogram of the neoplasms rate
ggplot(boot_data, aes(x = Deaths_NeoplasmsRate)) +
geom_histogram()
```

Visualizing the histograms for both the cardiovascular disease death rate and 
the neoplasms death rate it looks like both may follow a normal distribution.
Further hypothesis tests would have to be done to validate that they do follow
normal distributions. For our purposes, the visualization is good enough.

## Introduction

In this analysis, we develop and test two experiments with bootstrapping, a resampling method 
particularly useful when estimating the variance of the sample median. 

## Methods

The bootstrapping method is fairly simple to understand but results in a variety
of useful applications. Bootstrapping is a method of resampling that uses
random sampling with replacement to mirror the sampling process and allows
us to make inferences about the entire population based on just a sample from
the full population. In order to utilize the bootstrapping method, the first
step is to create $n$ number of new random samples from the current sample
by allowing replacement. This means that every observation in the sample has
the same probability of showing up in the new sample with every replacement.
There are a variety of different types of bootstrapping that determine how
to create the new samples. In the Monte Carlo method for case resampling, the
new samples must have the exact size as the original sample. Bayesian bootstrap
creates new samples by re-weighting the original sample. The smooth bootstrap
adds random noise to each observation. The parametric bootstrap fits a parametric
model to the original sample, where the new observations are pulled from the
model. These are just the surface of what bootstrapping methods are available. 
Once the new sample(s) are created, you can analyze each sample like you
would the original sample to gather information about how it performs. Finally,
you can make assumptions about the entire population based on the results of
the multiple samples. In our modeling, we will be using the Monte Carlo method 
for case resampling as it's the easiest to visualize and understand what is 
happening during the process. We will also use $n = 1000$ to give a good balance 
of the bootstrapping method and performance, although it can be scaled up to 
hundreds of thousands of samples. 

### Statistical Modeling

#### Estimating Population Mean and Standard Deviation

The first experiment we will apply the bootstrapping method is finding the
confidence intervals for the population mean and standard deviation. In order
to accomplish this task we need to follow the steps of the bootstrapping
method. Since we currently have an entire population worth of data, we will
take a sample of the population, $n = 20$. In this example we will also be looking
at only the cardiovascular diseases death rate.

```{r}
# Create our initial sample
boot_initial_sample <- sample(1:nrow(boot_data), 20, replace = FALSE)
```

Now that we have our original sample we can use bootstrapping case resampling
with the Monte Carlo method (new samples are the exact size as the original sample) 
to find 1,000 new samples using replacement. For each new sample created we
will save the mean and standard deviation.

```{r}
# Create vectors to store new sample means and standard deviations
boot_estimated_means <- rep()
boot_estimated_sds <- rep()
# Create 1,000 new samples and save the means and standard deviations
for (x in 1:1000) {
  boot_new_sample <- sample(boot_initial_sample, 20, replace = TRUE)
  boot_estimated_means <- append(boot_estimated_means,
                                 pull(summarize(boot_data[boot_new_sample,], mean(Deaths_CardiovascularDiseasesRate))))
  boot_estimated_sds <- append(boot_estimated_sds, 
                               pull(summarize(boot_data[boot_new_sample,], sd(Deaths_CardiovascularDiseasesRate))))
}
```

```{r}
# Display some estimated means
head(boot_estimated_means)
```

```{r}
# Display some estimated standard deviations
head(boot_estimated_sds)
```

Finally, now that we have the mean and standard deviations for all 1,000 samples
creating using bootstrapping, we can trim the top and bottom 2.5% to find our
95% confidence interval for the population mean and standard deviation.

```{r}
# Sort the estimated means from smallest to largest
boot_estimated_means <- sort(boot_estimated_means)
# Sort the estimated standard deviations from smallest to largest
boot_estimated_sds <- sort(boot_estimated_sds)
# Trim the top and bottom 2.5%
start = length(boot_estimated_means) * 0.025
end = length(boot_estimated_means) * 0.975
boot_estimated_means <- boot_estimated_means[start:end]
boot_estimated_sds <- boot_estimated_sds[start:end]
```

Now, by retrieving the first and last element in both the mean and standard
deviation vectors we will find our 95% estimated population intervals. The
95% confidence interval for the population mean is 
[`r round(boot_estimated_means[1], 4)`, `r round(boot_estimated_means[length(boot_estimated_means)], 4)`]
or [`r round(boot_estimated_means[1] * 100, 2)`%, `r round(boot_estimated_means[length(boot_estimated_means)] * 100, 2)`%].
The 95% confidence interval for the population standard deviation is 
[`r round(boot_estimated_sds[1], 4)`, `r round(boot_estimated_sds[length(boot_estimated_sds)], 4)`]
or [`r round(boot_estimated_sds[1] * 100, 2)`%, `r round(boot_estimated_sds[length(boot_estimated_sds)] * 100, 2)`%].

```{r}
# Display a histogram of the cardiovascular diseases deaths estimated population
# means rate
ggplot() +
geom_histogram(aes(boot_estimated_means))
```

#### Constructing Confidence Intervals on Regression Parameters

The next experiment we will apply the bootstrapping method to is finding the
confidence intervals for the regression parameters in a simple linear regression 
model to determine how variable a model is. Again, we will follow the steps of 
the bootstrapping method to achieve these results. To make things easier, we 
will use the sample initial sample created in the first experiment. For the 
model we will see if the deaths rate of neoplasms can be used to predict the 
deaths rate of cardiovascular diseases.

Again, we will use the Monte Carlo method to create $n = 1000$ new samples that
are the exact size of the initial sample. For each one of these samples we will
fit a simple linear regression model and save the coefficient and regression 
parameter.

```{r}
# Create vectors to store new intercepts and regression parameters
boot_estimated_intercepts <- rep()
boot_estimated_regressionparameters <- rep()
# Create 1,000 new samples and save the means and standard deviations
for (x in 1:1000) {
  boot_new_reg_sample <- sample(boot_initial_sample, 20, replace = TRUE)
  boot_new_lm <- lm(Deaths_CardiovascularDiseasesRate ~ Deaths_NeoplasmsRate,
                    boot_data[boot_new_sample,])
  boot_estimated_intercepts <- append(boot_estimated_intercepts, 
                                      boot_new_lm$coefficients[1])
  boot_estimated_regressionparameters <- append(boot_estimated_regressionparameters, 
                                                boot_new_lm$coefficients[2])
}
```

Lastly, using the intercepts and regression parameters found we can construct
our confidence intervals.

```{r}
# Sort the estimated means from smallest to largest
boot_estimated_intercepts <- sort(boot_estimated_intercepts)
# Sort the estimated standard deviations from smallest to largest
boot_estimated_regressionparameters <- sort(boot_estimated_regressionparameters)
# Trim the top and bottom 2.5%
start = length(boot_estimated_intercepts) * 0.025
end = length(boot_estimated_intercepts) * 0.975
boot_estimated_intercepts <- boot_estimated_intercepts[start:end]
boot_estimated_regressionparameters <- boot_estimated_regressionparameters[start:end]
```

Now, by retrieving the first and last element in both the intercept and 
regression parameter vectors we will find our 95% estimated population 
intervals. The 95% confidence interval for the intercept is 
[`r round(boot_estimated_intercepts[1], 4)`, `r round(boot_estimated_intercepts[length(boot_estimated_intercepts)], 4)`].
The 95% confidence interval for the regression parameter is 
[`r round(boot_estimated_regressionparameters[1], 4)`, `r round(boot_estimated_regressionparameters[length(boot_estimated_regressionparameters)], 4)`].

### Conlusion

Based on the results from the first experiment, estimating the population 
mean and standard deviation using the bootstrapping method, we can see that
the true population mean and standard deviation either fall within the 95% 
confidence interval or are close to it. In this case the true population mean
was `r round(boot_means$mean_cardiovasculardiseasesrate * 100, 2)`% and our 
confidence interval was [`r round(boot_estimated_means[1] * 100, 2)`%, `r round(boot_estimated_means[length(boot_estimated_means)] * 100, 2)`%].
Due to the nature of bootstrapping, the results may not always be perfect. Since
there is replacement, while unlikely, it's entirely possible that the smallest
value could be chosen to fill every single new sample. This would result in a
much lower estimated population mean than the true population mean. Similarly,
we bootstrapped for the estimate population standard deviation and ended up
with the 95% confidence interval [`r round(boot_estimated_sds[1] * 100, 2)`%, `r round(boot_estimated_sds[length(boot_estimated_sds)] * 100, 2)`%]
while the true population standard deviation is `r round(boot_means$sd_cardiovasculardiseasesrate * 100, 2)`%.

In the second experiment we simulated 1,000 simple linear regression models using
new samples created using the bootstrapping method to create a confidence interval
for the regression intercept and parameter to ultimately see if there is much
variability in the model. Our 95% confidence interval for the intercept is
[`r round(boot_estimated_intercepts[1], 4)`, `r round(boot_estimated_intercepts[length(boot_estimated_intercepts)], 4)`].
Our 95% confidence interval for the regression parameter is 
[`r round(boot_estimated_regressionparameters[1], 4)`, `r round(boot_estimated_regressionparameters[length(boot_estimated_regressionparameters)], 4)`].
We can see that the intervals are either the same number or extremely small. 
This tells us that the model has very little variability, or it is not easily
influenced by changing values. We can create the same simple linear regression
model using the full population data to see that the intercept and parameter 
are very close or identical to the bootstrapping intervals. Increasing the size
of our initial sample may increase the variability of the model.

```{r}
boot_lm <- lm(Deaths_CardiovascularDiseasesRate ~ Deaths_NeoplasmsRate,
              boot_data)
```

Fitting a model to the full dataset gives us a intercept of `r round(boot_lm$coefficients[1], 4)`
and a parameter value of `r round(boot_lm$coefficients[2], 4)`.

## References














