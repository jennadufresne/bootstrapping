---
title: "Bootstrapping"
format: revealjs
editor: visual
---

## Introduction

-   Resampling is the process of creating new samples based on an observed sample to gather more information about either the sample or the population the sample came from.
-   With resampling techniques you could verify the accuracy of the original sample, or make observations about the entire population of concert goers.
-   All of the resampling techniques listed either have assumptions required for them to make meaningful results, fail at specific estimations, or produce errors during the process.

## Bootstrap Methods

-   Bootstrapping is the process of taking a sample and using it to create a new sample with replacement.
-   Different version of the Bootstrap:
-   Monte Carlo technique takes exact copies of the data from the original sample to place into the new sample [@introduction]
-   Bayesian Bootstrap adds weights to each observation before selecting the data for the new sample.
-   This result in multiple different types of Bootstraps that can be used for different applications or to get around certain assumptions of another type of Bootstrap.

## Outcomes and Assumptions

-   The Importance of Discussing Assumptions when Teaching Bootstrapping [@assumptions] states the largest assumption listed is that the distribution can be made approximately pivotal through shifting or studentization. 
- Interesting result came out that if assumptions are broken when performing the Bootstrap method, it performs no better or worse then other methods whose assumptions are also broken. 
- Performing Bootstrapping with broken assumptions can also lead to decreased performance of the method.

## Continued
-   One of the shortcomings of the Bootstrap method is that the results still rely on the original sample [@introduction]. 
- if the original sample contains many outliers, then it most likely won't produce an accurate representation of the population. 
- Small sample sizes and data that does not follow a normal distribution has also been found to negatively impact the performance of Bootstrapping methods [@assumptions].

## Application 1

-   Bootstrapping has far reaching applications from finding a population mean to performing hypothesis testing. Initially, Efron introduced the Bootstrap method to estimate the sampling distribution, estimate the median, error rate estimation in discrimination analysis, Wilcoxon's statistic, and regression models [@efron1]. 

## Application 2
- In Nonparametric Estimates of Standard Error: The Jackknife, the Bootstrap, and Other Methods [@efron2]. He introduces the concept of using the Bootstrap to estimate the standard error based on the data. While this is normally done using parametric modeling methods, here it is being done with non-parametric methods, like the Bootstrap. As time has progressed, and computers have become more powerful, the applications of Bootstrapping have increased.

## Application 3
-   Bootstrapping to create confidence intervals for certain statistics. Due to the nature of Bootstrapping, it creates results that make perfect sense to construct confidence intervals. Often, Bootstrapping implements confidence intervals around estimated parameter values [@confidenceintervals]. One example is using Bootstrapping to find the population mean of a specific feature based on the sample means. A few other similar applications are creating a confidence interval for the population mean and estimating the distribution of a sample mean [@introduction].

## Application 4
-   Bootstrapping is in regression. In regression, the Bootstrap method can be used to perform validation on the model. In Bootstrapping with R to make generalized inference for regression model [@withr] the authors used the Bootstrap method to resample thousands of times, fit a model to each new sample, and save the intercept and regression coefficient estimates. Confidence intervals can then be created to assess performance of a population regression model against the Bootstrap sample models. Another possible application of Bootstrapping in regression is using it to approximate the distribution of the Lasso estimator for all possible values of the unknown regression parameter vector [@lasso].

## Application 5
-   The rise of statistical and mathematical programming languages and tools have greatly improved the state of Bootstrapping. Since Bootstrapping normally requires a large number of iterations to produce meaningful results, faster computers and tools have allowed statisticians to gain useful information. Software packages have been created for SAS, Stata [@difficult], and R [@withr] that allow common Bootstrapping functionality to be used readily and easily.

## Methods

1.  Construct the sample probability distribution $\hat{F}$, putting mass $1/n$ at each point $x_1, x_2, x_3, . . . , x_n$.
2.  With $\hat{F}$ fixed, draw a random sample of size $n$ from $\hat{F}$, say $X_{i}^{*} = x_{i}^{*}$, $X_{i}^{*} \sim _{ind}\hat{F}$ and call this the bootstrap sample.
3.  Approximate the sampling distribution of $R(X, \hat{F}$ by the bootstrap distribution of $R^{*} = R(X^{*}, \hat{F})$.

## Methods 

Or more simply, to perform the Bootstrap method as proposed, given a sample of size $n$, a new sample of size $n$ can be created by selecting from the original sample with replacement. Since the new sample is created with replacement, every element from the original sample has the same probability of ending up in the new sample. This can then by performed multiple times to estimate the sampling distribution based on the Bootstrap distribution.

## Types of Bootstrapping 

1. Monte Carlo Case resampling
2. Exact Case resampling
3. Smooth Bootstrap

## Monte Carlo Case resampling 

In the Monte Carlo method, a new sample is created by randomly selecting values 
from the original sample using replacement to create a sample of the same size. 
Statistics can then be computed from this new sample. This process is then 
repeated many times to create an estimate of the population statistic. [@efron1]

## Exact Case resampling 

The exact version for Bootstrap case resampling is similar to the Monte Carlo
method, except every possible enumeration of the initial sample is created. The
downside to this method is since there are a total of 
${2n - 1 \choose n} = \frac{(2n - 1)!}{n!(n - 1)!}$ possible samples, the 
process can be very intensive for large sample sizes. [@introduction]

## Smooth Bootstrap

- In the Smooth Bootstrap, a small amount of random noise is added to every resampled observation. This noise is zero-centered and usually normally distributed. Doing this means that the re-sampled data is not limited to just the data in the original sample, but rather to data points in close proximity to the original samples. Figure 1 shows an example of the simple bootstrap density function and Figure 2 shows an example of the smoothed density function provided by [@smooth].

![figure 1](Bootstrapping-Group-Report_files/figure-html/smooth1.png)
![figure 2](Bootstrapping-Group-Report_files/figure-html/smooth2.png)

## Data

The dataset being used to explore Bootstrapping is the 
"Causes of Death - Our World in Data" dataset from Kaggle [@chavez_2022] which was expanded with the World Bank population dataset [@worldbank]. The "Causes of Death - Our World in Data" dataset contains thirty three causes of death broken down by continent, region, country, and territory and by the year of reporting. The World Bank population dataset contains population numbers for multiple years also broken down by continent, region, country, and territory. The populations from the population dataset were added into the causes of death dataset to allow for the calculation of the rate of death, or percentage of total population, for each cause of death.

To complete the original dataset, all rows that did not correspond to countries were removed. The number of executions and terrorism columns were also removed from the original causes of death dataset due to a lack of data for most countries.

As a final note, the Vatican and Liechtenstein are the only two countries missing from the final, cleaned, dataset because they were not included in the original causes of death dataset from Kaggle.
